HOW THIS PORTFOLIO IS BUILT

Verifiable AI built with Python. I used a Python framework called Streamlit which converts standard Python code into this interactive web app. It handles the chat interface and document rendering dynamically. When you ask a question, your message is sent to Google Gemini (specifically the Gemma 2 27B model).
   - Before the AI answers, my system injects my resume and project data into its context window.
   - AI is instructed to answer only using facts found in that data.

To prevent "hallucinations" (AI making things up) and ensure explainability, I built a custom "Trace Engine".
   - After the AI generates text, my engine runs a search algorithm.
   - It compares the AI's words against my original documents.
   - If it finds an exact match, it highlights the text.
   - Clicking a highlight proves the text is grounded in reality.

Implements RAG pattern with a strict Post-Generation Verification layer.

System Architecture
 - Runtime: Python 3.11+ stateless implementation
 - Frontend: Streamlit Custom Component architecture
 - LLM Backend: Google GenAI SDK (`models/gemma-3-27b-it` via API)
   I choose this model as the optimal trade-off between Reasoning Density and Inference Efficiency:
   - 27B parameter scale provides sufficient capacity to strictly follow rigid negative constraints
   - Smaller models tend to paraphrase inputs, breaking verification. This model maintains high fidelity to source text while still synthesizing coherent answers.
   - Sufficient rate limits at 0 costs RPD: 14.4K, RPM: 30, TPM: 15K (only suffice 2 requests/min now due to RAG)
 - Deployment: Streamlit Cloud

Trace Engine
Core is the verification system located in `trace_engine.py`. NOT rely on embeddings or semantic similarity (fuzzy)
 - Extraction: PDF/DOCX files are ingested using `PyPDF2`/`python-docx` and normalized (whitespace collapsing) into a strictly clean text corpus.
 - Algorithm: The engine implements a Greedy Maximal Exact Match strategy (an optimized variation of well-known Longest Common Substring problem):
    - Iterates through the LLM's generated response character by character.
    - At each position `i`, it searches the entire Corpus `C` for the longest substring `S` starting at `i`.
    - Heuristics: Enforces word boundaries to prevent partial-word matching (e.g., matching "ring" inside "engineering").
    - If a match `|S| >= threshold` (e.g., 15 chars) is found, the engine injects an HTML anchor tag wrapping that span: `<a id="source_doc:::quote">...</a>`.

UI
 - Leveraging `st.session_state` to maintain chat history across re-runs.
 - Streamlit is declarative and re-runs the entire script on interaction. To handle the "Click-to-Open-Document" feature, we implement a custom Event Debouncing pattern. We track `clicked_states` to differentiate between a stale click signal and a fresh user intent, triggering `st.rerun()` only on state transitions.

Deployment
 - Storage: The application is stateless. User uploads during a session are stored in the ephemeral container filesystem (`/mount/src/...`) and are wiped upon session termination.
