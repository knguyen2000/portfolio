Adaptive Context GraphRAG Project Summary
OVERVIEW Retrieval-Augmented Generation (RAG) improves the factual reliability of large language models by conditioning generation on external knowledge sources rather than relying solely on parametric memory. Conventional RAG systems typically operate over unstructured text embeddings stored in vector databases, retrieving semantically similar text fragments for a given query. While effective for surface-level similarity matching, this approach struggles with complex queries that require relational reasoning, multi-hop inference, or contextual understanding beyond local semantic proximity.

Graph-based Retrieval-Augmented Generation (GraphRAG) addresses these limitations by representing knowledge as a graph composed of entities, attributes, and relationships. Graph structures encode relational dependencies explicitly, enabling retrieval mechanisms that exploit connectivity, hierarchy, and topology rather than embedding similarity alone. Microsoft GraphRAG introduced a scalable pipeline that constructs a knowledge graph from documents, partitions the graph using community detection algorithms such as Leiden, summarizes each community using a language model, and performs retrieval by selecting relevant communities during query time.

Despite its advantages, existing GraphRAG approaches exhibit several structural and semantic limitations. Community detection algorithms such as Leiden optimize purely for graph connectivity and modularity, without incorporating semantic similarity between nodes. As a result, nodes that are semantically related but weakly connected may be assigned to different communities, causing semantic fragmentation across partitions. This fragmentation introduces blind spots during retrieval, particularly for queries that span conceptual boundaries or depend on cross-community context. Additionally, Microsoft GraphRAG tightly couples indexing, summarization, and retrieval components, limiting flexibility in experimentation, optimization, and deployment. The reliance on paragraph-level summaries further reduces retrieval granularity, making it difficult to extract precise, node-level factual information.

The motivation of this project is to improve semantic coverage, retrieval precision, and architectural flexibility in GraphRAG systems without compromising the structural guarantees provided by graph-based community detection. Specifically, the project seeks to retain the strong connectivity properties of Leiden partitions while mitigating their semantic limitations, and to redesign the GraphRAG pipeline in a modular manner that supports fine-grained retrieval and efficient inference.

This work introduces Context-Aware Leiden, an extension to the Leiden community detection algorithm that augments structurally defined communities with semantically relevant context nodes. Rather than altering the original Leiden partitions, the method preserves all core community assignments and adds a bounded number of external nodes that exhibit strong semantic alignment with a given community despite lacking sufficient structural connectivity. These context nodes act as semantic bridges across communities, improving cross-cluster semantic continuity while maintaining structural integrity. Context selection is guided by node embeddings, similarity scoring, topological distance, and centrality-based penalties, with an adaptive budget to prevent excessive community expansion.

In parallel, the project proposes a modular GraphRAG pipeline that operates directly on knowledge graphs from the outset, rather than first converting documents into text chunks. The pipeline separates indexing, clustering, summarization, and retrieval into independently configurable stages. During retrieval, a lightweight LLM-based agent dynamically selects between two retrieval strategies: a local retriever that issues targeted Cypher queries for precise node-level facts, and a global retriever that leverages precomputed community summaries for broader contextual reasoning. This hybrid approach enables efficient handling of both narrow factual queries and high-level analytical questions while minimizing token usage and latency.

The central research statement of this project is that augmenting graph communities with semantically selected context nodes improves retrieval relevance and semantic coverage in GraphRAG systems, and that a modular, graph-native retrieval architecture enables more precise, efficient, and adaptable retrieval-augmented generation. The project evaluates this claim by comparing Context-Aware Leiden against standard Leiden using cross-cluster semantic similarity and Precision@5 metrics, and by qualitatively analyzing retrieval behavior relative to Microsoft GraphRAG. The results demonstrate consistent improvements in semantic connectivity and retrieval precision, validating the effectiveness of context-aware community augmentation and modular graph-centric retrieval design.

METHODOLOGY This project implements a modular GraphRAG pipeline that augments large language model retrieval using a context-aware adaptation of the Leiden community detection algorithm. The goal is to improve semantic coverage and retrieval precision while preserving the structural guarantees of graph-based partitioning.

The system begins with knowledge graph construction. Unstructured documents are processed by a Neo4j-based graph builder that extracts entities, attributes, and relationships using an LLM-assisted information extraction step. These elements are persisted as nodes and edges in a Neo4j graph. The resulting graph represents all source content in a structured, queryable format. For offline analytics, the graph is exported into memory as an edge list.

Community detection is performed using the Leiden algorithm. Leiden partitions the graph into well-connected communities by optimizing modularity while guaranteeing internal connectivity. These communities define local structural regions of the graph but are purely topology-driven and do not incorporate semantic information.

To address semantic blind spots, the project introduces Context-Aware Leiden, a post-processing extension that enriches each Leiden community with additional semantically aligned nodes called context nodes. The original Leiden partitions are preserved without modification.

Each node in the graph is embedded into a vector space. Structural embeddings are generated using Node2Vec. When textual attributes are available, semantic embeddings may also be generated using Sentence-BERT. These embeddings represent node content and structural roles in a comparable vector space.

For each node, similarity to every Leiden community is computed as the average cosine similarity between the node embedding and embeddings of nodes within the community. A node is considered semantically misaligned if its highest similarity score corresponds to a community different from its original Leiden assignment.

Candidate context nodes are scored using a function that combines semantic similarity and graph distance. Semantic similarity is squared to emphasize alignment strength, and the score is inversely weighted by shortest-path distance to the target community. This favors nodes that are both semantically relevant and topologically reachable.

To prevent globally central nodes from dominating context selection, each candidate score is penalized using normalized betweenness centrality. Nodes with high global connectivity are down-weighted to ensure selected context nodes represent meaningful cross-community semantic bridges rather than generic hubs.

Each community is assigned an adaptive context budget. The number of context nodes added to a community grows logarithmically with the number of misaligned candidates and is capped by a fixed maximum. This ensures bounded augmentation while allowing richer communities to incorporate more context.

For each community, the highest-scoring candidates within the budget are selected and attached as context nodes. These nodes remain members of their original Leiden communities but are also included in the target community, effectively creating semantic overlap without altering the underlying partition.

After context enrichment, community summaries are generated. All text associated with nodes in a community, including context nodes, is concatenated and summarized using an LLM. These summaries are stored back into Neo4j as community-level metadata.

The retrieval phase is query-adaptive. A lightweight LLM agent first classifies the user query as local or global. For local, fact-specific queries, the system issues targeted Cypher queries to retrieve precise nodes and relationships. For global or analytical queries, it retrieves precomputed community summaries. This separation minimizes token usage and latency.

Retrieved content is passed to an LLM strictly for answer generation. Graph traversal and selection are handled outside the model, ensuring clean separation between retrieval and reasoning.

All computationally expensive steps, including embeddings, Leiden clustering, context selection, and summarization, are performed offline during indexing. At query time, the system performs only lightweight graph queries and text assembly, enabling scalability and cost efficiency.

KEY RESULTS
Semantic Bridge Performance: Context nodes demonstrated a higher average cross-cluster similarity (0.2006) compared to original Leiden nodes (0.1784), confirming their role as semantic connectors.
Retrieval Precision: The Context-Aware Leiden approach achieved a Precision@5 score of 0.959, representing a 7.6% improvement over the baseline Leiden score of 0.882.
Operational Efficiency: The modular architecture reduced storage requirements by approximately 50% and lowered LLM token consumption by 40-60% by minimizing repeated summarization calls.

LIMITATIONS AND LEARNINGS While the project successfully demonstrated the potential of overlapping communities, several technical and methodological flaws were identified:
Heuristic Patching vs. Integrated Optimization: The approach functions as a post-processing heuristic that adds nodes to a frozen partition rather than integrating semantic similarity into a single, global optimization objective.
Embedding Dependency and Scale: Due to budget constraints, the implementation relied heavily on Node2Vec, which may not capture deep semantic nuances as effectively as full-text SBERT embeddings. The modest improvements were measured on a small graph of 100+ nodes, making the results on massive enterprise datasets speculative.
Indirect Evaluation Metrics: The project used proxy metrics like Precision@5 and cross-cluster similarity instead of evaluating end-to-end LLM answer quality, leaving the actual impact on user-facing responses unverified.
Redundancy and Noise: The duplication of nodes across clusters, intended as a semantic bridge, risks introducing information redundancy or distracting the LLM with tangential context in larger, noisier datasets.
Winner-Take-All Limitations: The current selector assigns a node to only one additional community based on the highest similarity, potentially overlooking multi-faceted nodes that should belong to multiple thematic groups.